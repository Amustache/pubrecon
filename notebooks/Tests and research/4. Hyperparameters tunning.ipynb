{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# !pip install -e ../  # If not done yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pubrecon.data import DataFrame, ImagesData\n",
    "from pubrecon.model import RCNN\n",
    "from pubrecon.hyper import Hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "data_path = \"../data/in/\"  # Where is your input data\n",
    "work_path = \"../data/out/\"  # Where everything will be saved\n",
    "seed = 1337  # Random seed\n",
    "verbose = 1  # 0: no output; 1: normal informations; 2: e v e r y th i n g\n",
    "\n",
    "# DataFrame\n",
    "dataframe_pickle_path = os.path.join(work_path, \"dataframe.pickle\")  # Where will the DataFrame be saved\n",
    "force_preparation = True  # Do you want to bypassed the saved DataFrame\n",
    "subsamples = 128  # Number of samples to use for the DataFrame; -1: Use all of them\n",
    "\n",
    "# ImagesData\n",
    "imagesdata_pickle_path = os.path.join(work_path, 'imagesdata.pickle')  # Where will the ImagesData be saved\n",
    "number_of_results = 2500  # How many samples will selective search use\n",
    "iou_threshold = 0.85  # What is the percent of precision required\n",
    "max_samples = 15  # How many class samples do you want\n",
    "show_infos = True  # Show information for images output\n",
    "show_labels = True  # Show labels for images output\n",
    "\n",
    "# RCNN\n",
    "model_and_weights_path = \"../data/out/\"  # Where will the model and weights be saved/loaded\n",
    "loss = None  # Loss function; None: Use \n",
    "opt = None  # Optimization function; None: Use Adame\n",
    "lr = 0.001  # Learning rate\n",
    "epochs = 5  # Number of epochs\n",
    "batch_size = 16\n",
    "split_size = 0.15  # Test/Train proportion\n",
    "checkpoint_path = os.path.join(work_path, 'checkpoint.h5')  # Where will the checkpoints be saved; None: No checkpoint (don't.)\n",
    "early_stopping = True  # Should the learning stop if no more improvment is done\n",
    "threshold = 0.85  # Threshold used for the recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = DataFrame(data_path, pickle_path=dataframe_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.prepare_data(force_preparation=force_preparation, subsamples=subsamples, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesdata = ImagesData(dataframe, pickle_path=imagesdata_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That part is quite long, beware!\n",
    "imagesdata.prepare_images_and_labels(number_of_results=number_of_results, iou_threshold=iou_threshold,\n",
    "                                     max_samples=max_samples, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = imagesdata.images\n",
    "y = imagesdata.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save(\"x\", x)\n",
    "np.save(\"y\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"x.npy\")\n",
    "y = np.load(\"y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rcnn(x_train, y_train, x_val, y_val, param):\n",
    "    vggmodel = VGG16(weights='imagenet', include_top=True)  # https://keras.io/applications/\n",
    "\n",
    "    # Freeze first 15 layers\n",
    "    for i, layers in enumerate(vggmodel.layers[:15]):\n",
    "        layers.trainable = False\n",
    "\n",
    "    # Add a {number of classes} unit softmax dense layer\n",
    "    predictions = Dense(len(set(y_train)), activation=\"softmax\")(\n",
    "        vggmodel.layers[-2].output)  # Maybe not all labels\n",
    "    model = Model(input=vggmodel.input, output=predictions)\n",
    "\n",
    "    # Compile the model using Adam optimizer with learning rate of 0.001 by default\n",
    "    # We are using categorical_crossentropy as loss by default since the output of the model is categorical\n",
    "    model.compile(loss=param['loss'], optimizer=param['opt'](lr=lr_normalizer(param['lr'], param['opt'])), metrics=[\"acc\"])\n",
    "    \n",
    "    class MyLabelBinarizer(LabelBinarizer):\n",
    "        def transform(self, y):\n",
    "            Y = super().transform(y)\n",
    "            if self.y_type_ == 'binary':\n",
    "                return np.hstack((Y, 1 - Y))\n",
    "            else:\n",
    "                return Y\n",
    "\n",
    "        def inverse_transform(self, Y, threshold=None):\n",
    "            if self.y_type_ == 'binary':\n",
    "                return super().inverse_transform(Y[:, 0], threshold)\n",
    "            else:\n",
    "                return super().inverse_transform(Y, threshold)\n",
    "\n",
    "    chosen_binarizer = MyLabelBinarizer()\n",
    "    train_labels_fit = chosen_binarizer.fit_transform(y_train)\n",
    "    classes = chosen_binarizer.classes_\n",
    "\n",
    "    # Dataset augmentation\n",
    "    # This may not be needed following some magazines, as we do not often have rotated texts...\n",
    "    # ... Or do we? Anyway it applies for the pictures so there's that.\n",
    "    imgdatagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "    train_data = imgdatagen.flow(x=x_train, y=y_train, batch_size=param['batch_size'])\n",
    "    imgdatagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "    test_data = imgdatagen.flow(x=x_val, y=y_val, batch_size=param['batch_size'])\n",
    "    \n",
    "    print(train_datata)\n",
    "\n",
    "    # FINALLY train the model. https://keras.io/models/sequential/#fit_generator\n",
    "    steps = ceil(len(train_data) / param['batch_size'])\n",
    "    hist = model.fit(x_train, y_train, batch_size=param['batch_size'], steps_per_epoch=steps, epochs=param['epochs'], verbose=0,\n",
    "                                         validation_data=[x_val, y_val], validation_steps=steps)\n",
    "    \n",
    "    return hist, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {'loss': ['categorical_crossentropy'],\n",
    "          'lr': (0.1, 10, 10),\n",
    "          'epochs': [200],\n",
    "          'opt': [Adam, Nadam],\n",
    "          'batch_size': [2, 3, 4],\n",
    "          'split_size': [0.1, 0.15],\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.image.numpy_array_iterator.NumpyArrayIterator object at 0x7f036062e990>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_10 to have shape (25,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-54ce61a837b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtalos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'diabetes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/project2/lib/python3.7/site-packages/talos/scan/Scan.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, params, model, experiment_name, x_val, y_val, val_split, random_method, seed, performance_target, fraction_limit, round_limit, time_limit, boolean_limit, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, minimize_loss, disable_progress_bar, print_params, clear_session, save_weights)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# start runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mscan_run\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mscan_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/project2/lib/python3.7/site-packages/talos/scan/scan_run.py\u001b[0m in \u001b[0;36mscan_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# otherwise proceed with next permutation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mscan_round\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/project2/lib/python3.7/site-packages/talos/scan/scan_round.py\u001b[0m in \u001b[0;36mscan_round\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mingest_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/project2/lib/python3.7/site-packages/talos/model/ingest_model.py\u001b[0m in \u001b[0;36mingest_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                       self.round_params)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-975bfb4a6921>\u001b[0m in \u001b[0;36mrcnn\u001b[0;34m(x_train, y_train, x_val, y_val, param)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     hist = model.fit(x_train, y_train, batch_size=param['batch_size'], steps_per_epoch=steps, epochs=param['epochs'], verbose=0,\n\u001b[0;32m---> 48\u001b[0;31m                                          validation_data=[x_val, y_val], validation_steps=steps)\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/project2/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/project2/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/project2/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_10 to have shape (25,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "t = talos.Scan(x=x, y=y, params=p, model=rcnn, experiment_name='diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from talos.utils import lr_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_object = talos.Analyze(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the highest result for any metric\n",
    "analyze_object.high('val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the round with the best result\n",
    "analyze_object.rounds2high('val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best paramaters\n",
    "analyze_object.best_params('val_acc', ['acc', 'loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# heatmap correlation\n",
    "analyze_object.plot_corr('val_loss', ['acc', 'loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_train, y_train, x_val, y_val, params):\n",
    "    vggmodel = VGG16(weights='imagenet', include_top=True)  # https://keras.io/applications/\n",
    "\n",
    "    # Freeze first 15 layers\n",
    "    for i, layers in enumerate(vggmodel.layers[:15]):\n",
    "        layers.trainable = False\n",
    "\n",
    "    # Add a {number of classes} unit softmax dense layer\n",
    "    predictions = Dense(imagesdata.get_num_classes(), activation=\"softmax\")(\n",
    "        vggmodel.layers[-2].output)  # Maybe not all labels\n",
    "    self.model = Model(input=vggmodel.input, output=predictions)\n",
    "\n",
    "    # Compile the model using Adam optimizer with learning rate of 0.001 by default\n",
    "    # We are using categorical_crossentropy as loss by default since the output of the model is categorical\n",
    "    self.model.compile(loss=param['loss'], optimizer=param['opt'], metrics=[\"acc\"])\n",
    "    \n",
    "    class MyLabelBinarizer(LabelBinarizer):\n",
    "        def transform(self, y):\n",
    "            Y = super().transform(y)\n",
    "            if self.y_type_ == 'binary':\n",
    "                return np.hstack((Y, 1 - Y))\n",
    "            else:\n",
    "                return Y\n",
    "\n",
    "        def inverse_transform(self, Y, threshold=None):\n",
    "            if self.y_type_ == 'binary':\n",
    "                return super().inverse_transform(Y[:, 0], threshold)\n",
    "            else:\n",
    "                return super().inverse_transform(Y, threshold)\n",
    "\n",
    "    chosen_binarizer = MyLabelBinarizer()\n",
    "    train_labels_fit = chosen_binarizer.fit_transform(labels)\n",
    "    self.classes = chosen_binarizer.classes_\n",
    "\n",
    "    # Dataset augmentation\n",
    "    # This may not be needed following some magazines, as we do not often have rotated texts...\n",
    "    # ... Or do we? Anyway it applies for the pictures so there's that.\n",
    "    imgdatagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "    train_data = imgdatagen.flow(x=x_train, y=y_train, batch_size=batch_size)\n",
    "    imgdatagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "    test_data = imgdatagen.flow(x=x_test, y=y_test, batch_size=batch_size)\n",
    "\n",
    "    # FINALLY train the model. https://keras.io/models/sequential/#fit_generator\n",
    "    steps = ceil(len(train_data) / param['batch_size'])\n",
    "    self.hist = self.model.fit_generator(generator=train_data, steps_per_epoch=steps, epochs=param['epochs'], verbose=0,\n",
    "                                         validation_data=test_data, validation_steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_object = talos.Scan(images,\n",
    "                         labels, \n",
    "                         model=model,\n",
    "                         params=params,\n",
    "                         experiment_name='aaa',\n",
    "                         fraction_limit=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
