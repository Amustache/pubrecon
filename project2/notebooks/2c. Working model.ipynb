{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,cv2,keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are we going to work?\n",
    "data_path = '../data/gen4/'\n",
    "work_path = '../data/work/'\n",
    "pickle_path = os.path.join(work_path, 'out.pickle')\n",
    "checkpoint_path = os.path.join(work_path, 'checkpoint_{epoch:02d}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Force data preparation\n",
    "force_conversion = True\n",
    "\n",
    "# Only use a few examples\n",
    "subsamples = False\n",
    "subsamples_num = 10\n",
    "\n",
    "# Recognition\n",
    "N = 2000\n",
    "IOU_THRESHOLD = 0.8\n",
    "MAX_SAMPLES = 30  # We need to balance the numbers of examples for each class\n",
    "\n",
    "# Training\n",
    "STEPS_PER_EPOCH=10\n",
    "EPOCHS=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation/retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(pickle_path) or force_conversion:  # If we need to convert data\n",
    "    # Collect files\n",
    "    names = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for name in files:\n",
    "            names.append(os.path.join(root, name.split('.')[0]))\n",
    "    names = list(set(names))  # We only want the files once - discard .jpg/.xml duality\n",
    "    \n",
    "    # Columns\n",
    "    columns = ['file_name', 'class_name', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "\n",
    "    # Data\n",
    "    data = []\n",
    "    for name in names:\n",
    "        xml_path = name + '.xml'\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "        except:  # The annotation file is missing.\n",
    "            continue\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            temp = [name + '.jpg', obj.find('name').text]\n",
    "            for child in obj.find('bndbox'):\n",
    "                temp.append(child.text)\n",
    "            data.append(temp)\n",
    "        \n",
    "        if subsamples:\n",
    "            if subsamples_num > 0:\n",
    "                subsamples_num -= 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # Create a new pandas dataframe\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.head()\n",
    "    \n",
    "    # Save pickle\n",
    "    df.to_pickle(pickle_path)\n",
    "else:  # The data is already available\n",
    "    df = pd.read_pickle(pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See files\n",
    "files = df.groupby('file_name').size().reset_index(name='counts')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See classes\n",
    "classes = df.groupby('class_name').size().reset_index(name='counts').sort_values(by='counts', ascending=False)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See numbers\n",
    "# Why use len(foo.index)? https://stackoverflow.com/a/15943975\n",
    "num_classes = len(classes.index)\n",
    "num_files = len(files.index)\n",
    "num_pod = len(df.index)\n",
    "print(\"We have {} classes in {} files for {} points of data.\".format(num_classes, num_files, num_pod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row = files.sample(1, random_state=SEED)\n",
    "sample_file = sample_row.iloc[0, 0]\n",
    "sample_data = df.loc[df['file_name'] == sample_file]\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(sample_file)\n",
    "plt.imshow(img)\n",
    "for index, row in sample_data.iterrows():\n",
    "    class_name, xmin, ymin, xmax, ymax = row['class_name'], int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n",
    "    cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (255,0,0), 2)\n",
    "    cv2.putText(img, class_name, (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA) \n",
    "plt.figure()\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognition and data expension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.setUseOptimized(True)  # Enables the optimized code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using selective search to find potential areas of interest.\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()  # Uses optimized selective search (https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/)\n",
    "img = cv2.imread(sample_file)\n",
    "ss.setBaseImage(img)\n",
    "print(\"Processing Selective Search...\")\n",
    "ss.switchToSelectiveSearchFast()\n",
    "rects = ss.process()\n",
    "img_out = img.copy()\n",
    "for i, rect in (enumerate(rects)):\n",
    "    x, y, w, h = rect\n",
    "    cv2.rectangle(img_out, (x, y), (x + w, y + h), (0, 255, 0), 1, cv2.LINE_AA)\n",
    "plt.figure()\n",
    "plt.imshow(img_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses optimized selective search (https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/)\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(bb1, bb2):  # Intersection over Union (IoU) (https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)\n",
    "    # Basically, Area of Intersection / Area of Union...\n",
    "    # bb1, bb2: {xmin, ymin, xmax, ymax}\n",
    "    assert bb1['xmin'] < bb1['xmax']\n",
    "    assert bb1['ymin'] < bb1['ymax']\n",
    "    assert bb2['xmin'] < bb2['xmax']\n",
    "    assert bb2['ymin'] < bb2['ymax']\n",
    "\n",
    "    x_left = max(bb1['xmin'], bb2['xmin'])\n",
    "    y_top = max(bb1['ymin'], bb2['ymin'])\n",
    "    x_right = min(bb1['xmax'], bb2['xmax'])\n",
    "    y_bottom = min(bb1['ymax'], bb2['ymax'])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    bb1_area = (bb1['xmax'] - bb1['xmin']) * (bb1['ymax'] - bb1['ymin'])\n",
    "    bb2_area = (bb2['xmax'] - bb2['xmin']) * (bb2['ymax'] - bb2['ymin'])\n",
    "\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop over each image\n",
    "for index, row in tqdm(files.iterrows(), total=num_files, desc=\"Iterating through files...\"):\n",
    "    file = row[0]\n",
    "    data = df.loc[df['file_name'] == file]\n",
    "    img = cv2.imread(file)\n",
    "    \n",
    "    # Set image as the base for selective search\n",
    "    ss.setBaseImage(img)\n",
    "    \n",
    "    # Initialising fast selective search and getting proposed regions\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "    rects = ss.process()\n",
    "    \n",
    "    img_out = img.copy()\n",
    "    \n",
    "    classes_counter = defaultdict(int)  # As stated, we need an uniform sample between classes\n",
    "    \n",
    "    # Iterate over the first N results of selective search\n",
    "    # Calculate IOU of proposed region and annoted region\n",
    "    used = False  # Check if that bbox is used as a class example\n",
    "    \n",
    "    # For each rectangle in the results of selective search\n",
    "    for i, rect in enumerate(tqdm(rects, desc=\"Iterating through rectangles...\", leave=False)):\n",
    "        if i < N:  # We don't want to waste ressources on too many possibilities.\n",
    "            x, y, w, h = rect\n",
    "            rect_bbox = {'xmin': x, 'xmax': x + w, 'ymin': y, 'ymax': y + h}\n",
    "            \n",
    "            # For each bbox within the image\n",
    "            for index, row in data.iterrows():\n",
    "                ground_truth_bbox = {'xmin': int(row['xmin']), 'xmax': int(row['xmax']),\n",
    "                                     'ymin': int(row['ymin']), 'ymax': int(row['ymax'])}\n",
    "                ground_truth_class_name = row['class_name']\n",
    "                \n",
    "                # Compare them\n",
    "                iou = get_iou(ground_truth_bbox, rect_bbox)\n",
    "                \n",
    "                if iou > IOU_THRESHOLD and classes_counter[ground_truth_class_name] < MAX_SAMPLES:\n",
    "                    img_sample = cv2.resize(img_out[y:y + h, x:x + w], (224,224), interpolation = cv2.INTER_AREA)  # Get the sample\n",
    "                    train_images.append(img_sample)\n",
    "                    train_labels.append(ground_truth_class_name)\n",
    "                    classes_counter[ground_truth_class_name] += 1\n",
    "                    used = True\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            if not used and classes_counter['background'] < MAX_SAMPLES: # We can use that bbox as a background example!\n",
    "                img_sample = cv2.resize(img_out[y:y + h, x:x + w], (224, 224), interpolation = cv2.INTER_AREA)  # Get the sample\n",
    "                train_images.append(img_sample)\n",
    "                train_labels.append('background')  # Background\n",
    "                classes_counter['background'] += 1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "We are going to use transfer learning, aka \"We do not have time to train a whole new model for a while so let's cut through an existing model and specialize it\". https://medium.com/@1297rohit/transfer-learning-from-scratch-using-keras-339834b153b9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import pretrained original VGG16 model with ImageNet weights\n",
    "vggmodel = VGG16(weights='imagenet', include_top=True)  # https://keras.io/applications/\n",
    "vggmodel.summary()  # Pretty sure I can optimize that thing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze first 15 layers\n",
    "for i, layers in enumerate(vggmodel.layers[:15]):\n",
    "    layers.trainable = False\n",
    "    print(\"- Layer {} ({}) is not trainable anymore.\".format(layers.get_config()['name'], i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a {number of classes} unit softmax dense layer\n",
    "predictions = Dense(len(set(train_labels)), activation=\"softmax\")(vggmodel.layers[-2].output)  # Maybe not all labels\n",
    "model = Model(input = vggmodel.input, output = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using Adam optimizer with learning rate of 0.001\n",
    "# We are using categorical_crossentropy as loss since the output of the model is categorical\n",
    "opt = Adam(lr=0.0001)\n",
    "model.compile(loss = keras.losses.categorical_crossentropy, optimizer = opt, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding: Basically \"unique-fy-ish\" each class.\n",
    "# https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "class MyLabelBinarizer(LabelBinarizer):\n",
    "    def transform(self, y):\n",
    "        Y = super().transform(y)\n",
    "        if self.y_type_ == 'binary':\n",
    "            return np.hstack((Y, 1-Y))\n",
    "        else:\n",
    "            return Y\n",
    "    def inverse_transform(self, Y, threshold=None):\n",
    "        if self.y_type_ == 'binary':\n",
    "            return super().inverse_transform(Y[:, 0], threshold)\n",
    "        else:\n",
    "            return super().inverse_transform(Y, threshold)\n",
    "        \n",
    "lenc = MyLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_binarizer = lenc\n",
    "train_labels_fit = chosen_binarizer.fit_transform(train_labels)\n",
    "# train_labels_fit = chosen_binarizer.fit_transform([train_labels])  # For mlb\n",
    "chosen_binarizer.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and train set, yay.\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_images, train_labels_fit, test_size=0.10)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset augmentation\n",
    "# This may not be needed following some magazines, as we do not often have rotated texts...\n",
    "# ... Or do we? Anyway it applies for the pictures so there's that.\n",
    "imgdatagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "train_data = imgdatagen.flow(x=X_train, y=y_train)\n",
    "imgdatagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "test_data = imgdatagen.flow(x=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want checkpoints because losing training suckz lolz. https://keras.io/callbacks/#modelcheckpoint\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# If we are not doing any progress, stops the whole thing. https://keras.io/callbacks/#earlystopping\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINALLY train the model. https://keras.io/models/sequential/#fit_generator\n",
    "hist = model.fit_generator(generator=train_data, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS,\n",
    "                           verbose=1, validation_data=test_data,\n",
    "                           validation_steps=2, callbacks=[checkpoint, early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moment of (ground) truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Loss\", \"Validation Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    test_img = X_test[random.randint(0, num_files)]\n",
    "    plt.figure()\n",
    "    test_out = np.expand_dims(test_img, axis=0)\n",
    "    test_out = model.predict(test_out)\n",
    "    guess = chosen_binarizer.classes_[test_out.argmax()]\n",
    "    guess_probability = test_out.max()\n",
    "    plt.title(\"{} with probability {}.\".format(guess, guess_probability))\n",
    "    plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
