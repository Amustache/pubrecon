{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# !pip install -e ../  # If not done yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pubrecon.data import DataFrame, ImagesData\n",
    "from pubrecon.model import RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "data_path = \"../data/in/\"  # Where is your input data\n",
    "work_path = \"../data/out/\"  # Where everything will be saved\n",
    "seed = 1337  # Random seed\n",
    "verbose = 1  # 0: no output; 1: normal informations; 2: e v e r y th i n g\n",
    "\n",
    "# DataFrame\n",
    "dataframe_pickle_path = os.path.join(work_path, \"dataframe.pickle\")  # Where will the DataFrame be saved\n",
    "force_preparation = True  # Do you want to bypassed the saved DataFrame\n",
    "subsamples = 512  # Number of samples to use for the DataFrame; -1: Use all of them\n",
    "\n",
    "# ImagesData\n",
    "imagesdata_pickle_path = os.path.join(work_path, 'imagesdata.pickle')  # Where will the ImagesData be saved\n",
    "number_of_results = 2500  # How many samples will selective search use\n",
    "iou_threshold = 0.85  # What is the percent of precision required\n",
    "max_samples = 30  # How many class samples do you want\n",
    "show_infos = False  # Show information for images output\n",
    "show_labels = False  # Show labels for images output\n",
    "\n",
    "# RCNN\n",
    "model_and_weights_path = \"../data/out/\"  # Where will the model and weights be saved/loaded\n",
    "loss = None  # Loss function; None: Use crossentropy\n",
    "opt = None  # Optimization function; None: Use Adame\n",
    "lr = 0.001  # Learning rate\n",
    "epochs = 10  # Number of epochs\n",
    "batch_size = 16\n",
    "split_size = 0.10  # Test/Train proportion\n",
    "checkpoint_path = os.path.join(work_path, 'checkpoint.h5')  # Where will the checkpoints be saved; None: No checkpoint (don't.)\n",
    "early_stopping = False  # Should the learning stop if no more improvment is done\n",
    "threshold = 0.80  # Threshold used for the recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you did not generate data yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = DataFrame(data_path, pickle_path=dataframe_pickle_path)\n",
    "# dataframe.prepare_data(force_preparation=force_preparation, subsamples=subsamples, verbose=verbose)\n",
    "# dataframe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagesdata = ImagesData(dataframe, pickle_path=imagesdata_pickle_path)\n",
    "# # That part is quite long, beware!\n",
    "# imagesdata.prepare_images_and_labels(number_of_results=number_of_results, iou_threshold=iou_threshold,\n",
    "#                                      max_samples=max_samples, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # Save it for later.\n",
    "# with open(imagesdata_pickle_path, 'wb') as fi:\n",
    "#     pickle.dump(imagesdata, fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(imagesdata_pickle_path, 'rb') as fi:\n",
    "    imagesdata = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesdata.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "losses = ['categorical_crossentropy']\n",
    "optimizers = [Adam, Nadam]\n",
    "lrs = [0.001 * x for x in range(1, 11)]\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "\n",
    "num_tests = len(losses) * len(optimizers) * len(lrs) * len(batch_sizes)\n",
    "print(\"Total tests: {}.\".format(num_tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tests = []\n",
    "test = 0\n",
    "for o in tqdm(optimizers):\n",
    "    for l in tqdm(lrs, leave=False):\n",
    "        for b in tqdm(batch_sizes, leave=False):\n",
    "            test += 1\n",
    "            print(\"test {}\".format(test))\n",
    "            print(o, l, b)\n",
    "            model = RCNN(imagesdata, loss=None, opt=o, lr=l, verbose=0)\n",
    "            model.train(epochs=5, batch_size=b, split_size=split_size, checkpoint_path=None,\n",
    "                        early_stopping=None, verbose=1)\n",
    "            tests.append([o, l, b, model.history()])\n",
    "            \n",
    "            with open(os.path.join(work_path, \"tests.pickle\"), 'wb') as fi:\n",
    "                pickle.dump(tests, fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
